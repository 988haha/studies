Transformer/BERT 系列模型
===
<!--START_SECTION:badge-->

![last modify](https://img.shields.io/static/v1?label=last%20modify&message=2022-10-12%2001:12:13&color=yellowgreen&style=flat-square)

<!--END_SECTION:badge-->

- [背景](#背景)
- [常见面试问题](#常见面试问题)

## 背景
- 原始 Transformer 指的是一个基于 Encoder-Decoder 框架的 Seq2Seq 模型，用于解决机器翻译任务；
- 后其 Encoder 部分被用于 BERT 而广为人知，因此有时 Transformer 也特指其 Encoder 部分；
- 相关论文：
    - [[1706.03762] Attention Is All You Need](https://arxiv.org/abs/1706.03762)
    - [[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)


## 常见面试问题
> [Transformer 常见面试问题](./Transformer常见面试问题.md)
